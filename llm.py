#!/usr/bin/env python3
"""
ç®€åŒ–çš„ LLM å‘½ä»¤è¡Œå·¥å…·
æ”¯æŒ: llm pull, llm run, llm list
"""

import sys
import argparse
from model_manager import ModelManager

class SimpleLLM:
    def __init__(self):
        self.manager = ModelManager()
    
    def pull(self, model_name):
        """æ‹‰å–æ¨¡å‹"""
        print(f"ğŸš€ æ­£åœ¨æ‹‰å–æ¨¡å‹: {model_name}")
        try:
            result = self.manager.pull_model(model_name)
            if result.get('success'):
                print(f"âœ… æ¨¡å‹ {model_name} æ‹‰å–æˆåŠŸ!")
            else:
                print(f"âŒ æ¨¡å‹æ‹‰å–å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        except Exception as e:
            print(f"âŒ æ‹‰å–å¤±è´¥: {str(e)}")
    
    def list_models(self):
        """åˆ—å‡ºæ‰€æœ‰æ¨¡å‹"""
        print("ğŸ“‹ å·²ä¸‹è½½çš„æ¨¡å‹:")
        try:
            result = self.manager.list_models()
            models = result.get('models', {})
            
            if not models:
                print("  æš‚æ— å·²ä¸‹è½½çš„æ¨¡å‹")
                print("  ä½¿ç”¨ 'llm pull <model_name>' ä¸‹è½½æ¨¡å‹")
                return
            
            for i, (model_name, model_info) in enumerate(models.items(), 1):
                status = "âœ… å·²åŠ è½½" if model_info.get('loaded') else "â­• æœªåŠ è½½"
                print(f"  {i}. {model_name} - {status}")
                if model_info.get('size'):
                    print(f"     å¤§å°: {model_info['size']}")
                if model_info.get('path'):
                    print(f"     è·¯å¾„: {model_info['path']}")
        except Exception as e:
            print(f"âŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}")
    
    def delete(self, model_name):
        """åˆ é™¤æ¨¡å‹"""
        print(f"ğŸ—‘ï¸  æ­£åœ¨åˆ é™¤æ¨¡å‹: {model_name}")
        
        # ç¡®è®¤åˆ é™¤
        try:
            confirm = input(f"âš ï¸  ç¡®å®šè¦åˆ é™¤æ¨¡å‹ '{model_name}' å—ï¼Ÿè¿™å°†åˆ é™¤æ‰€æœ‰ç›¸å…³æ–‡ä»¶ã€‚(y/N): ").strip().lower()
            if confirm not in ['y', 'yes', 'æ˜¯']:
                print("âŒ å–æ¶ˆåˆ é™¤")
                return
            
            result = self.manager.delete_model(model_name)
            if result.get('error'):
                print(f"âŒ åˆ é™¤å¤±è´¥: {result['error']}")
            else:
                print(f"âœ… æ¨¡å‹ {model_name} åˆ é™¤æˆåŠŸ!")
                print("ğŸ’¡ æç¤º: ä½¿ç”¨ 'llm list' æŸ¥çœ‹å‰©ä½™æ¨¡å‹")
        except KeyboardInterrupt:
            print("\nâŒ å–æ¶ˆåˆ é™¤")
        except Exception as e:
            print(f"âŒ åˆ é™¤å¤±è´¥: {str(e)}")
    
    def generate(self, model_name, prompt, max_tokens=100, temperature=0.7):
        """å•æ¬¡æ–‡æœ¬ç”Ÿæˆ"""
        print(f"ğŸš€ å•æ¬¡ç”Ÿæˆæ¨¡å¼")
        print(f"ğŸ¤– æ¨¡å‹: {model_name}")
        print(f"ğŸ“ æç¤º: {prompt}")
        print("=" * 50)
        
        try:
            # åŠ è½½æ¨¡å‹
            result = self.manager.load_model(model_name)
            if not result.get('success'):
                print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {result.get('error')}")
                return
            
            # ç”Ÿæˆæ–‡æœ¬
            response = self.manager.generate_text(
                model_name=model_name,
                prompt=prompt,
                max_tokens=max_tokens,
                temperature=temperature
            )
            
            if response.get('success'):
                print("ğŸ¤– ç”Ÿæˆç»“æœ:")
                print(response['response'])
            else:
                print(f"âŒ ç”Ÿæˆå¤±è´¥: {response.get('error')}")
                
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}")

    def run(self, model_name=None):
        """è¿è¡Œäº¤äº’å¼èŠå¤©"""
        if not model_name:
            # è·å–ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
            result = self.manager.list_models()
            models = result.get('models', {})
            if not models:
                print("âŒ æ²¡æœ‰å¯ç”¨æ¨¡å‹ï¼Œè¯·å…ˆä½¿ç”¨ 'llm pull <model_name>' ä¸‹è½½æ¨¡å‹")
                return
            model_name = list(models.keys())[0]
        
        print(f"ğŸš€ å¯åŠ¨æ¨¡å‹: {model_name}")
        
        try:
            # åŠ è½½æ¨¡å‹
            result = self.manager.load_model(model_name)
            if not result.get('success'):
                print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {result.get('error')}")
                return
            
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
            print("ğŸ’¬ å¼€å§‹èŠå¤© (è¾“å…¥ 'exit' é€€å‡º)")
            print("=" * 50)
            
            # å¯¹è¯å†å²
            messages = []
            
            while True:
                try:
                    user_input = input("\nğŸ‘¤ ä½ : ").strip()
                    
                    if user_input.lower() in ['exit', 'quit', 'bye', 'é€€å‡º']:
                        print("ğŸ‘‹ å†è§!")
                        break
                    
                    if not user_input:
                        continue
                    
                    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
                    messages.append({"role": "user", "content": user_input})
                    
                    # ä¿æŒå¯¹è¯å†å²åœ¨åˆç†é•¿åº¦
                    if len(messages) > 10:
                        messages = messages[-8:]
                    
                    print("ğŸ¤– AI: ", end="", flush=True)
                    
                    # æµå¼ç”Ÿæˆå›å¤
                    full_reply = ""
                    try:
                        for chunk in self.manager.chat_completion_stream(model_name, messages):
                            if not chunk.get('success'):
                                print(f"âŒ ç”Ÿæˆå¤±è´¥: {chunk.get('error')}")
                                break
                            
                            content = chunk.get('content', '')
                            if content:
                                print(content, end="", flush=True)
                                full_reply += content
                            
                            # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                            if chunk.get('done'):
                                full_reply = chunk.get('full_response', full_reply)
                                break
                        
                        print()  # æ¢è¡Œ
                        
                        if full_reply:
                            # æ·»åŠ AIå›å¤åˆ°å†å²
                            messages.append({"role": "assistant", "content": full_reply})
                        
                    except Exception as e:
                        print(f"\nâŒ æµå¼ç”Ÿæˆé”™è¯¯: {str(e)}")
                        # å›é€€åˆ°éæµå¼æ¨¡å¼
                        response = self.manager.chat_completion(model_name, messages)
                        if response.get('success'):
                            reply = response['response']
                            print(reply)
                            messages.append({"role": "assistant", "content": reply})
                        else:
                            print(f"âŒ ç”Ÿæˆå¤±è´¥: {response.get('error')}")
                
                except KeyboardInterrupt:
                    print("\nğŸ‘‹ å†è§!")
                    break
                except Exception as e:
                    print(f"\nâŒ é”™è¯¯: {str(e)}")
        
        except Exception as e:
            print(f"âŒ è¿è¡Œå¤±è´¥: {str(e)}")

def main():
    parser = argparse.ArgumentParser(
        description="ç®€åŒ–çš„ LLM å‘½ä»¤è¡Œå·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  llm pull microsoft/Phi-3-mini-4k-instruct-gguf    # æ‹‰å–æ¨¡å‹
  llm list                                           # åˆ—å‡ºæ¨¡å‹
  llm delete <model_name>                            # åˆ é™¤æ¨¡å‹
  llm run                                           # è¿è¡Œç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
  llm run Qwen/Qwen2-1.5B-Instruct-GGUF            # è¿è¡ŒæŒ‡å®šæ¨¡å‹
  llm generate <model> "ä½ å¥½"                        # å•æ¬¡ç”Ÿæˆæ–‡æœ¬
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # pull å‘½ä»¤
    pull_parser = subparsers.add_parser('pull', help='æ‹‰å–æ¨¡å‹')
    pull_parser.add_argument('model', help='æ¨¡å‹åç§° (ä¾‹: microsoft/Phi-3-mini-4k-instruct-gguf)')
    
    # list å‘½ä»¤
    subparsers.add_parser('list', help='åˆ—å‡ºå·²ä¸‹è½½çš„æ¨¡å‹')
    
    # delete å‘½ä»¤
    delete_parser = subparsers.add_parser('delete', help='åˆ é™¤æ¨¡å‹')
    delete_parser.add_argument('model', help='è¦åˆ é™¤çš„æ¨¡å‹åç§°')
    
    # run å‘½ä»¤
    run_parser = subparsers.add_parser('run', help='è¿è¡Œäº¤äº’å¼èŠå¤©')
    run_parser.add_argument('model', nargs='?', help='æ¨¡å‹åç§° (å¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹)')
    
    # generate å‘½ä»¤
    generate_parser = subparsers.add_parser('generate', help='å•æ¬¡æ–‡æœ¬ç”Ÿæˆ')
    generate_parser.add_argument('model', help='æ¨¡å‹åç§°')
    generate_parser.add_argument('prompt', help='è¾“å…¥æç¤ºæ–‡æœ¬')
    generate_parser.add_argument('--max-tokens', type=int, default=32768, help='æœ€å¤§ç”Ÿæˆtokenæ•° (é»˜è®¤: 32768)')
    generate_parser.add_argument('--temperature', type=float, default=0.7, help='æ¸©åº¦å‚æ•° (é»˜è®¤: 0.7)')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    llm = SimpleLLM()
    
    if args.command == 'pull':
        llm.pull(args.model)
    elif args.command == 'list':
        llm.list_models()
    elif args.command == 'delete':
        llm.delete(args.model)
    elif args.command == 'run':
        llm.run(args.model)
    elif args.command == 'generate':
        llm.generate(args.model, args.prompt, args.max_tokens, args.temperature)

if __name__ == "__main__":
    main()