# Python LLM - Local Large Language Model Service

English Version | [ä¸­æ–‡ç‰ˆ](./README.md)

A lightweight local large language model service, similar to Ollama, supporting GGUF format model pulling, management, and inference.

## ğŸŒŸ Features

- **ğŸš€ Quick Deployment**: One-click startup with no complex configuration
- **ğŸ“¦ GGUF Support**: Native support for quantized GGUF format models
- **ğŸ¯ Multi-Interface**: Support for API, CLI, and web interface
- **âš¡ Hardware Acceleration**: Auto-detect and use Metal (macOS) or CUDA (Linux/Windows)
- **ğŸ”„ Streaming Response**: Support for streaming text generation
- **ğŸ’¾ Memory Optimization**: Intelligent model loading and memory management

## ğŸš€ Quick Start

### Install Dependencies

```bash
# Clone project
git clone <repository-url>
cd python-llm

# Install dependencies
pip install -r requirements.txt
```

### Start Service

```bash
# Start API service
python main.py

# Or use startup script
./start.sh
```

Service will start at http://localhost:8000, visit /docs for API documentation.

### Command Line Usage

```bash
# Pull model
./llm pull microsoft/Phi-3-mini-4k-instruct-gguf

# List models
./llm list

# Run interactive chat
./llm run microsoft/Phi-3-mini-4k-instruct-gguf

# Single text generation
./llm generate microsoft/Phi-3-mini-4k-instruct-gguf "Hello, please introduce machine learning"
```

## ğŸ“– Detailed Usage

### API Endpoints

#### 1. Pull Model
```bash
curl -X POST "http://localhost:8000/models/pull" \
     -H "Content-Type: application/json" \
     -d '{"model_name": "microsoft/Phi-3-mini-4k-instruct-gguf"}'
```

#### 2. Text Generation
```bash
curl -X POST "http://localhost:8000/generate" \
     -H "Content-Type: application/json" \
     -d '{
       "model_name": "microsoft/Phi-3-mini-4k-instruct-gguf",
       "prompt": "Hello",
       "max_tokens": 512,
       "temperature": 0.7
     }'
```

#### 3. Chat Completion
```bash
curl -X POST "http://localhost:8000/generate/chat" \
     -H "Content-Type: application/json" \
     -d '{
       "model_name": "microsoft/Phi-3-mini-4k-instruct-gguf",
       "messages": [
         {"role": "user", "content": "Hello"}
       ],
       "max_tokens": 512
     }'
```

### Recommended Models

| Model Name | Size | Use Case |
|------------|------|----------|
| `microsoft/Phi-3-mini-4k-instruct-gguf` | 2.4GB | General conversation |
| `Qwen/Qwen2-1.5B-Instruct-GGUF` | 3.2GB | Chinese conversation |
| `unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF` | 1.5GB | Lightweight inference |
| `unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF` | 8GB | High-quality inference |

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `HOST` | 0.0.0.0 | Service listening address |
| `PORT` | 8000 | Service port |
| `USE_GPU` | True | Whether to use GPU acceleration |
| `LOG_LEVEL` | INFO | Log level |

## ğŸ—ï¸ Project Structure

```
python-llm/
â”œâ”€â”€ main.py              # Main program entry
â”œâ”€â”€ model_manager.py     # Model manager
â”œâ”€â”€ inference.py         # Inference engine
â”œâ”€â”€ config.py           # Configuration file
â”œâ”€â”€ llm.py              # Command line tool
â”œâ”€â”€ llm                 # Command line entry script
â”œâ”€â”€ api/                # API module
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models.py       # Model management API
â”‚   â””â”€â”€ generate.py     # Text generation API
â”œâ”€â”€ utils/              # Utility module
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ download.py     # Model downloader
â”œâ”€â”€ models/             # Model storage directory
â”‚   â””â”€â”€ models_info.json # Model information file
â””â”€â”€ requirements.txt    # Dependencies list
```

## ğŸ”§ Development Guide

### Adding New Features

1. **Add new API endpoints**: Create new route files in the `api/` directory
2. **Extend model support**: Modify model selection logic in `utils/download.py`
3. **Custom configuration**: Add new configuration items in `config.py`

### Debug Mode

```bash
# Enable debug mode
export DEBUG=true
python main.py

# View detailed logs
export LOG_LEVEL=DEBUG
python main.py
```

## ğŸ› Common Issues

### Model Download Failure
- **Check network**: Ensure access to huggingface.co
- **Proxy settings**: Set HTTP_PROXY/HTTPS_PROXY environment variables
- **Retry**: Delete corresponding folder in models directory and retry

### Insufficient Memory
- **Use smaller models**: Choose 1.5B or smaller models
- **Quantized models**: Use Q4_0 or Q8_0 quantized versions
- **Disable GPU**: Set USE_GPU=false to use CPU

### Slow Inference Speed
- **Check device**: Check logs to confirm GPU acceleration is being used
- **Model caching**: First load is slower, subsequent loads will be cached
- **Adjust parameters**: Reduce max_tokens or use faster sampling parameters

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ¤ Contributing

Welcome to submit Issues and Pull Requests!

## ğŸ“ Support

For help, please use the following methods:
1. Check project documentation
2. Run test scripts
3. Submit GitHub Issue